{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":3920225,"sourceType":"datasetVersion","datasetId":2327988}],"dockerImageVersionId":30732,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Climate Data Analysis\n\nUndertake a comprehensive climate data analysis project to explore and\nunderstand historical climate patterns and trends. The objective is to derive\nvaluable insights from climate data, enabling a better understanding of weather\nconditions over time.\n","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt","metadata":{"execution":{"iopub.status.busy":"2024-06-12T07:04:14.739027Z","iopub.execute_input":"2024-06-12T07:04:14.739477Z","iopub.status.idle":"2024-06-12T07:04:15.237118Z","shell.execute_reply.started":"2024-06-12T07:04:14.739428Z","shell.execute_reply":"2024-06-12T07:04:15.235954Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"hourly = pd.read_csv('/kaggle/input/temperature-data-albany-new-york/hourly_data.csv')\nhourly.head()","metadata":{"execution":{"iopub.status.busy":"2024-06-12T07:04:16.233539Z","iopub.execute_input":"2024-06-12T07:04:16.234096Z","iopub.status.idle":"2024-06-12T07:04:16.710341Z","shell.execute_reply.started":"2024-06-12T07:04:16.23406Z","shell.execute_reply":"2024-06-12T07:04:16.709128Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"daily = pd.read_csv('/kaggle/input/temperature-data-albany-new-york/daily_data.csv')\ndaily.head()","metadata":{"execution":{"iopub.status.busy":"2024-06-12T07:04:34.395453Z","iopub.execute_input":"2024-06-12T07:04:34.395876Z","iopub.status.idle":"2024-06-12T07:04:34.454397Z","shell.execute_reply.started":"2024-06-12T07:04:34.395844Z","shell.execute_reply":"2024-06-12T07:04:34.453257Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"monthly = pd.read_csv('/kaggle/input/temperature-data-albany-new-york/monthly_data.csv')\nmonthly.head()","metadata":{"execution":{"iopub.status.busy":"2024-06-12T07:04:41.430616Z","iopub.execute_input":"2024-06-12T07:04:41.431717Z","iopub.status.idle":"2024-06-12T07:04:41.473708Z","shell.execute_reply.started":"2024-06-12T07:04:41.431676Z","shell.execute_reply":"2024-06-12T07:04:41.472237Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"three_hour = pd.read_csv('/kaggle/input/temperature-data-albany-new-york/three_hour_data.csv')\nthree_hour.head()","metadata":{"execution":{"iopub.status.busy":"2024-06-12T07:04:54.076656Z","iopub.execute_input":"2024-06-12T07:04:54.077235Z","iopub.status.idle":"2024-06-12T07:04:54.245788Z","shell.execute_reply.started":"2024-06-12T07:04:54.077194Z","shell.execute_reply":"2024-06-12T07:04:54.244396Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"hourly.columns","metadata":{"execution":{"iopub.status.busy":"2024-06-12T07:05:13.322339Z","iopub.execute_input":"2024-06-12T07:05:13.322754Z","iopub.status.idle":"2024-06-12T07:05:13.33301Z","shell.execute_reply.started":"2024-06-12T07:05:13.322723Z","shell.execute_reply":"2024-06-12T07:05:13.330151Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"daily.columns","metadata":{"execution":{"iopub.status.busy":"2024-06-12T07:05:22.680503Z","iopub.execute_input":"2024-06-12T07:05:22.681927Z","iopub.status.idle":"2024-06-12T07:05:22.692359Z","shell.execute_reply.started":"2024-06-12T07:05:22.681873Z","shell.execute_reply":"2024-06-12T07:05:22.690439Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"monthly.columns","metadata":{"execution":{"iopub.status.busy":"2024-06-12T07:05:36.345996Z","iopub.execute_input":"2024-06-12T07:05:36.346616Z","iopub.status.idle":"2024-06-12T07:05:36.355851Z","shell.execute_reply.started":"2024-06-12T07:05:36.34656Z","shell.execute_reply":"2024-06-12T07:05:36.354541Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"three_hour.columns","metadata":{"execution":{"iopub.status.busy":"2024-06-12T07:05:50.26369Z","iopub.execute_input":"2024-06-12T07:05:50.264093Z","iopub.status.idle":"2024-06-12T07:05:50.27358Z","shell.execute_reply.started":"2024-06-12T07:05:50.264064Z","shell.execute_reply":"2024-06-12T07:05:50.271729Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"hourly.info()","metadata":{"execution":{"iopub.status.busy":"2024-06-12T07:06:39.626971Z","iopub.execute_input":"2024-06-12T07:06:39.627381Z","iopub.status.idle":"2024-06-12T07:06:39.732991Z","shell.execute_reply.started":"2024-06-12T07:06:39.627349Z","shell.execute_reply":"2024-06-12T07:06:39.731433Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"daily.info()","metadata":{"execution":{"iopub.status.busy":"2024-06-12T07:06:28.592393Z","iopub.execute_input":"2024-06-12T07:06:28.592834Z","iopub.status.idle":"2024-06-12T07:06:28.620595Z","shell.execute_reply.started":"2024-06-12T07:06:28.5928Z","shell.execute_reply":"2024-06-12T07:06:28.619325Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"monthly.info()","metadata":{"execution":{"iopub.status.busy":"2024-06-12T07:06:49.351041Z","iopub.execute_input":"2024-06-12T07:06:49.351488Z","iopub.status.idle":"2024-06-12T07:06:49.36778Z","shell.execute_reply.started":"2024-06-12T07:06:49.351441Z","shell.execute_reply":"2024-06-12T07:06:49.366431Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"three_hour.info()","metadata":{"execution":{"iopub.status.busy":"2024-06-12T07:07:06.788241Z","iopub.execute_input":"2024-06-12T07:07:06.788652Z","iopub.status.idle":"2024-06-12T07:07:06.827373Z","shell.execute_reply.started":"2024-06-12T07:07:06.788624Z","shell.execute_reply":"2024-06-12T07:07:06.825691Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"hourly.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2024-06-12T07:07:28.430244Z","iopub.execute_input":"2024-06-12T07:07:28.4309Z","iopub.status.idle":"2024-06-12T07:07:28.547765Z","shell.execute_reply.started":"2024-06-12T07:07:28.430864Z","shell.execute_reply":"2024-06-12T07:07:28.545997Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"daily.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2024-06-12T07:07:51.237655Z","iopub.execute_input":"2024-06-12T07:07:51.238109Z","iopub.status.idle":"2024-06-12T07:07:51.251227Z","shell.execute_reply.started":"2024-06-12T07:07:51.238075Z","shell.execute_reply":"2024-06-12T07:07:51.250072Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"monthly.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2024-06-12T07:08:06.650439Z","iopub.execute_input":"2024-06-12T07:08:06.650887Z","iopub.status.idle":"2024-06-12T07:08:06.662266Z","shell.execute_reply.started":"2024-06-12T07:08:06.650854Z","shell.execute_reply":"2024-06-12T07:08:06.660998Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"three_hour.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2024-06-12T07:08:34.751351Z","iopub.execute_input":"2024-06-12T07:08:34.751788Z","iopub.status.idle":"2024-06-12T07:08:34.78608Z","shell.execute_reply.started":"2024-06-12T07:08:34.751757Z","shell.execute_reply":"2024-06-12T07:08:34.784629Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**No Missing Value in all of the datasets**","metadata":{}},{"cell_type":"code","source":"hourly.shape, daily.shape, monthly.shape, three_hour.shape","metadata":{"execution":{"iopub.status.busy":"2024-06-12T07:10:33.042956Z","iopub.execute_input":"2024-06-12T07:10:33.043367Z","iopub.status.idle":"2024-06-12T07:10:33.052577Z","shell.execute_reply.started":"2024-06-12T07:10:33.043335Z","shell.execute_reply":"2024-06-12T07:10:33.051355Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"To undertake a comprehensive Exploratory Data Analysis (EDA) project for the climate data, you need to outline a series of tasks. Here's a structured approach to guide you through the process:\n\n### 1. Project Setup\n\n**1.1 Define Objectives**\n- Clarify the goals and questions you aim to answer through the analysis.\n- Define the key insights you wish to derive from the climate data.\n\n**1.2 Environment Setup**\n- Install necessary libraries (e.g., pandas, numpy, matplotlib, seaborn).\n- Set up the project directory structure (data, scripts, notebooks, reports).\n\n### 2. Data Collection and Loading\n\n**2.1 Data Acquisition**\n- Gather all four datasets (hourly, daily, monthly, three-hour).\n\n**2.2 Data Loading**\n- Load the datasets into pandas DataFrames.\n- Ensure proper parsing of dates and times.\n\n### 3. Data Cleaning and Preprocessing\n\n**3.1 Data Cleaning**\n- Handle missing values:\n  - Decide on strategies for handling missing data (e.g., imputation, removal).\n- Standardize column names and formats across datasets.\n- Correct any data entry errors or inconsistencies.\n\n**3.2 Data Integration**\n- Align the datasets on common columns (e.g., STATION, DATE).\n- Merge datasets where necessary to create comprehensive views (e.g., merging hourly and three-hour data).\n\n**3.3 Feature Engineering**\n- Create new features if necessary (e.g., daily temperature range, humidity index).\n- Aggregate data to different levels of granularity if required (e.g., from hourly to daily averages).\n\n### 4. Exploratory Data Analysis (EDA)\n\n**4.1 Descriptive Statistics**\n- Calculate and visualize summary statistics for key variables.\n- Explore distributions of key metrics (e.g., temperature, precipitation).\n\n**4.2 Data Visualization**\n- Time series plots for temperature, precipitation, humidity, etc.\n- Heatmaps for correlation analysis.\n- Box plots and histograms for distribution analysis.\n- Geographic visualizations if location data is relevant.\n\n**4.3 Trend Analysis**\n- Identify and analyze trends over time (e.g., seasonal patterns, long-term trends).\n- Perform decomposition of time series data to separate trend, seasonality, and residuals.\n\n**4.4 Anomaly Detection**\n- Identify and visualize anomalies or outliers in the data.\n- Investigate potential causes for anomalies.\n\n### 5. Advanced Analysis\n\n**5.1 Correlation and Causation**\n- Explore correlations between different climate variables.\n- Use statistical methods to test for causation where applicable.\n\n**5.2 Predictive Modeling**\n- Build simple predictive models (e.g., linear regression) to forecast future climate trends.\n- Validate models using cross-validation techniques.\n\n### 6. Reporting and Documentation\n\n**6.1 Results Interpretation**\n- Summarize key findings and insights from the EDA.\n- Highlight significant patterns, trends, and anomalies.\n\n**6.2 Visual Report**\n- Create comprehensive visual reports using tools like Jupyter Notebooks.\n- Use storytelling with data to make the report understandable and engaging.\n\n**6.3 Documentation**\n- Document all steps, decisions, and methodologies used in the analysis.\n- Ensure the code and analysis are reproducible.\n\n### 7. Final Presentation\n\n**7.1 Presentation Preparation**\n- Prepare a presentation summarizing the key findings, insights, and recommendations.\n- Use visual aids and clear explanations to communicate results.\n\n**7.2 Stakeholder Engagement**\n- Present findings to relevant stakeholders.\n- Discuss potential implications and actions based on the analysis.\n\n### 8. Future Work and Improvements\n\n**8.1 Feedback Integration**\n- Gather feedback from stakeholders and peers.\n- Identify areas for improvement or further analysis.\n\n**8.2 Continuous Monitoring**\n- Set up processes for continuous monitoring and updating of the analysis as new data becomes available.\n\n**8.3 Advanced Techniques**\n- Explore more advanced analytical techniques (e.g., machine learning models, deep learning) if necessary.\n\nBy following these steps, you'll ensure a thorough and comprehensive exploratory data analysis project that provides valuable insights into historical climate patterns and trends.\n","metadata":{}},{"cell_type":"markdown","source":"Sure, let's dive deeper into steps 3 through 5, covering data cleaning and preprocessing, exploratory data analysis (EDA), and advanced analysis in detail.\n\n### 3. Data Cleaning and Preprocessing\n\n#### 3.1 Data Cleaning\n\n**3.1.1 Handle Missing Values**\n- **Identify Missing Data**: Use `isnull()` and `sum()` functions to find missing values in each column.\n  ```python\n  hourly.isnull().sum()\n  daily.isnull().sum()\n  monthly.isnull().sum()\n  three_hour.isnull().sum()\n  ```\n- **Imputation**: Decide on strategies for imputing missing data. Common methods include:\n  - Filling with mean, median, or mode.\n  - Forward fill or backward fill for time series data.\n  ```python\n  hourly['HourlyPrecipitation'].fillna(hourly['HourlyPrecipitation'].mean(), inplace=True)\n  daily.fillna(method='ffill', inplace=True)\n  ```\n- **Removal**: If a column has too many missing values, consider removing it.\n  ```python\n  monthly.drop(columns=['SomeColumn'], inplace=True)\n  ```\n\n**3.1.2 Standardize Column Names and Formats**\n- Ensure column names are consistent across datasets.\n  ```python\n  hourly.columns = hourly.columns.str.lower()\n  daily.columns = daily.columns.str.lower()\n  monthly.columns = monthly.columns.str.lower()\n  three_hour.columns = three_hour.columns.str.lower()\n  ```\n\n**3.1.3 Correct Data Entry Errors**\n- Check for and correct any inconsistencies or errors in the data (e.g., out-of-range values).\n\n#### 3.2 Data Integration\n\n**3.2.1 Align Datasets**\n- Ensure common columns are formatted similarly.\n  ```python\n  hourly['date'] = pd.to_datetime(hourly['date'])\n  daily['date'] = pd.to_datetime(daily['date'])\n  monthly['date'] = pd.to_datetime(monthly['date'])\n  three_hour['date'] = pd.to_datetime(three_hour['date'])\n  ```\n\n**3.2.2 Merge Datasets**\n- Merge datasets on common columns like 'station' and 'date'.\n  ```python\n  merged_data = pd.merge(hourly, daily, on=['station', 'date'], how='outer')\n  merged_data = pd.merge(merged_data, monthly, on=['station', 'date'], how='outer')\n  merged_data = pd.merge(merged_data, three_hour, on=['station', 'date'], how='outer')\n  ```\n\n#### 3.3 Feature Engineering\n\n**3.3.1 Create New Features**\n- Derive new features from existing data to enhance analysis.\n  ```python\n  daily['temperature_range'] = daily['dailymaximumdrybulbtemperature'] - daily['dailyminimumdrybulbtemperature']\n  ```\n\n**3.3.2 Aggregate Data**\n- Aggregate data to different granularities if required.\n  ```python\n  daily_avg = hourly.resample('D', on='date').mean()\n  ```\n\n### 4. Exploratory Data Analysis (EDA)\n\n#### 4.1 Descriptive Statistics\n\n**4.1.1 Summary Statistics**\n- Calculate summary statistics for key variables.\n  ```python\n  hourly.describe()\n  daily.describe()\n  monthly.describe()\n  three_hour.describe()\n  ```\n\n#### 4.2 Data Visualization\n\n**4.2.1 Time Series Plots**\n- Visualize key metrics over time.\n  ```python\n  plt.figure(figsize=(10, 6))\n  plt.plot(daily['date'], daily['dailyaveragedrybulbtemperature'])\n  plt.title('Daily Average Dry Bulb Temperature Over Time')\n  plt.xlabel('Date')\n  plt.ylabel('Temperature')\n  plt.show()\n  ```\n\n**4.2.2 Correlation Heatmap**\n- Explore relationships between variables.\n  ```python\n  plt.figure(figsize=(12, 8))\n  sns.heatmap(daily.corr(), annot=True, cmap='coolwarm')\n  plt.title('Correlation Heatmap')\n  plt.show()\n  ```\n\n**4.2.3 Box Plots and Histograms**\n- Analyze the distribution of key variables.\n  ```python\n  sns.boxplot(x='dailyaveragedrybulbtemperature', data=daily)\n  plt.title('Box Plot of Daily Average Dry Bulb Temperature')\n  plt.show()\n\n  daily['dailyaveragedrybulbtemperature'].hist(bins=30)\n  plt.title('Histogram of Daily Average Dry Bulb Temperature')\n  plt.show()\n  ```\n\n**4.2.4 Geographic Visualizations**\n- If location data is relevant, visualize data geographically.\n  ```python\n  import geopandas as gpd\n  world = gpd.read_file(gpd.datasets.get_path('naturalearth_lowres'))\n  ax = world.plot(figsize=(15, 10))\n  gdf = gpd.GeoDataFrame(daily, geometry=gpd.points_from_xy(daily.backuplongitude, daily.backuplatitude))\n  gdf.plot(ax=ax, color='red', markersize=5)\n  plt.show()\n  ```\n\n#### 4.3 Trend Analysis\n\n**4.3.1 Seasonal Decomposition**\n- Decompose time series to analyze trend, seasonality, and residuals.\n  ```python\n  from statsmodels.tsa.seasonal import seasonal_decompose\n  result = seasonal_decompose(daily['dailyaveragedrybulbtemperature'], model='additive', period=365)\n  result.plot()\n  plt.show()\n  ```\n\n#### 4.4 Anomaly Detection\n\n**4.4.1 Identify Outliers**\n- Detect and visualize anomalies in the data.\n  ```python\n  from scipy import stats\n  z_scores = stats.zscore(daily['dailyaveragedrybulbtemperature'])\n  abs_z_scores = np.abs(z_scores)\n  outliers = daily[abs_z_scores > 3]\n  plt.plot(daily['date'], daily['dailyaveragedrybulbtemperature'])\n  plt.scatter(outliers['date'], outliers['dailyaveragedrybulbtemperature'], color='red')\n  plt.show()\n  ```\n\n### 5. Advanced Analysis\n\n#### 5.1 Correlation and Causation\n\n**5.1.1 Explore Correlations**\n- Investigate relationships between different climate variables.\n  ```python\n  sns.pairplot(daily[['dailyaveragedrybulbtemperature', 'dailyaveragehumidity', 'dailyprecipitation']])\n  plt.show()\n  ```\n\n**5.1.2 Causation Analysis**\n- Use statistical methods to explore causation.\n  ```python\n  from statsmodels.tsa.stattools import grangercausalitytests\n  grangercausalitytests(daily[['dailyprecipitation', 'dailyaveragedrybulbtemperature']], maxlag=5)\n  ```\n\n#### 5.2 Predictive Modeling\n\n**5.2.1 Build Predictive Models**\n- Develop models to forecast future climate trends.\n  ```python\n  from sklearn.model_selection import train_test_split\n  from sklearn.linear_model import LinearRegression\n  X = daily[['dailyaveragedewpointtemperature', 'dailyaveragehumidity', 'dailyprecipitation']]\n  y = daily['dailyaveragedrybulbtemperature']\n  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n  model = LinearRegression()\n  model.fit(X_train, y_train)\n  y_pred = model.predict(X_test)\n  ```\n\n**5.2.2 Model Validation**\n- Validate models using cross-validation techniques.\n  ```python\n  from sklearn.model_selection import cross_val_score\n  scores = cross_val_score(model, X, y, cv=5)\n  print('Cross-Validation Scores:', scores)\n  ```\n\nBy following these detailed steps, you can effectively clean, preprocess, explore, and analyze the climate data, gaining valuable insights into historical climate patterns and trends.","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}