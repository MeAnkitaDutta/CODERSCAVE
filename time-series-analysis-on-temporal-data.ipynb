{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":433077,"sourceType":"datasetVersion","datasetId":195545}],"dockerImageVersionId":30746,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom statsmodels.tsa.seasonal import seasonal_decompose\nfrom statsmodels.tsa.stattools import adfuller\nfrom statsmodels.graphics.tsaplots import plot_acf, plot_pacf\nfrom statsmodels.tsa.arima.model import ARIMA\nfrom sklearn.metrics import mean_squared_error","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data = pd.read_csv('/kaggle/input/amzn-dpz-btc-ntfx-adjusted-may-2013may2019/portfolio_data.csv')\ndata.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.isnull().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.columns","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 1. Data Loading and Exploration\n# Load the dataset\n# data_path = \"/kaggle/input/amzn-dpz-btc-ntfx-adjusted-may-2013may2019/portfolio_data.csv\"  # Change this to your dataset path\n# df = pd.read_csv(data_path)\n\n\n# Convert 'Date' column to datetime format and set as index\ndata['Date'] = pd.to_datetime(data['Date'])\ndata.set_index('Date', inplace=True)\n\n# Display the first few rows of the dataset\ndata.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check for missing values\nmissing_values = data.isna().sum()\nprint(missing_values)\n\n# Handle missing values (if any)\ndata = data.dropna()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 3. Data Visualization\n# Plot the time series for each stock\nplt.figure(figsize=(14, 10))\nfor i, column in enumerate(data.columns, 1):\n    plt.subplot(len(data.columns), 1, i)\n    plt.plot(data[column])\n    plt.title(f'Time Series for {column}')\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 4. Decomposition of Time Series\n# Decompose each time series\ndecompositions = {}\nfor column in data.columns:\n    decompositions[column] = seasonal_decompose(data[column], model='multiplicative', period=365)\n    decompositions[column].plot()\n    plt.title(f'Decomposition of {column}')\n    plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 5. Statistical Analysis\n# Summary statistics\nsummary_statistics = data.describe()\nprint(summary_statistics)\n\n# Calculate and plot moving averages\nplt.figure(figsize=(14, 10))\nfor i, column in enumerate(data.columns, 1):\n    plt.subplot(len(data.columns), 1, i)\n    plt.plot(data[column], label='Original')\n    plt.plot(data[column].rolling(window=30).mean(), label='30-Day Moving Average')\n    plt.title(f'Moving Averages for {column}')\n    plt.legend()\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 6. Stationarity Testing\ndef adf_test(series):\n    result = adfuller(series)\n    print(f'ADF Statistic: {result[0]}')\n    print(f'p-value: {result[1]}')\n    for key, value in result[4].items():\n        print(f'Critical Value {key}: {value}')\n\n# Perform ADF test\nfor column in data.columns:\n    print(f'Stationarity Test for {column}')\n    adf_test(data[column])\n    print('\\n')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 7. Autocorrelation and Partial Autocorrelation Analysis\nplt.figure(figsize=(14, 10))\nfor i, column in enumerate(data.columns, 1):\n    plt.subplot(len(data.columns), 2, 2*i-1)\n    plot_acf(data[column], ax=plt.gca(), title=f'ACF for {column}')\n    plt.subplot(len(data.columns), 2, 2*i)\n    plot_pacf(data[column], ax=plt.gca(), title=f'PACF for {column}')\nplt.tight_layout()\nplt.show()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 8. Time Series Modeling\n# Split the data into training and testing sets\ntrain_size = int(len(data) * 0.8)\ntrain, test = data.iloc[:train_size], data.iloc[train_size:]\n\n# Fit ARIMA model for each stock\nmodels = {}\nfor column in data.columns:\n    model = ARIMA(train[column], order=(5, 1, 0))\n    models[column] = model.fit()\n    print(f'{column} ARIMA Model Summary')\n    print(models[column].summary())\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 9. Forecasting\n# Forecasting for each stock\nforecasts = {}\nfor column in data.columns:\n    start = len(train)\n    end = len(train) + len(test) - 1\n    forecasts[column] = models[column].predict(start=start, end=end, typ='levels')\n    plt.figure(figsize=(10, 6))\n    plt.plot(test[column], label='Actual')\n    plt.plot(forecasts[column], label='Forecast')\n    plt.title(f'Forecast vs Actual for {column}')\n    plt.legend()\n    plt.show()\n\n    # Calculate RMSE\n    rmse = np.sqrt(mean_squared_error(test[column], forecasts[column]))\n    print(f'{column} RMSE: {rmse}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}